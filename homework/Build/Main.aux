\relax 
\catcode `"\active 
\catcode `-\active 
\select@language{czech}
\@writefile{toc}{\select@language{czech}}
\@writefile{lof}{\select@language{czech}}
\@writefile{lot}{\select@language{czech}}
\select@language{american}
\@writefile{toc}{\select@language{american}}
\@writefile{lof}{\select@language{american}}
\@writefile{lot}{\select@language{american}}
\citation{ARTHUR1994}
\citation{CHALLET2005}
\citation{CHALLET1997}
\citation{MORO2004}
\citation{CHALLET2005}
\citation{JOHNSON2003}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\newlabel{sec:introduction}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Mathematical Model of the Minority Game}{1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Strategy example for $m=3$.}}{2}}
\newlabel{tab:strategy_table_example}{{1}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Learning in the Minority Game}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Standard Learning Mechanism}{2}}
\newlabel{subsec:original}{{3.1}{2}}
\citation{ANDRECUT2002}
\citation{WHITEHEAD2008}
\newlabel{eq:mg_score_upd}{{4}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Q-learning Mechanism}{3}}
\newlabel{subsec:q}{{3.2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Roth-Erev Learning Mechanism}{3}}
\newlabel{subsec:rerl}{{3.3}{3}}
\citation{WHITEHEAD2008}
\@writefile{toc}{\contentsline {section}{\numberline {4}Simulations}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Performance of agent with standard learning mechanism.}}{5}}
\newlabel{fig:figure_1}{{1}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Strategy Portfolio dynamics of agent with standard learning mechanism.}}{5}}
\newlabel{fig:figure_2}{{2}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Performance of agent with Q-learning mechanism (standard learning process was switched off for it at $t_{0}=250$), $\alpha =\genfrac  {}{}{}1{1}{2^{m}}$, $\gamma =0.75$, $\varepsilon =\genfrac  {}{}{}1{250}{t}$ where $t\in \{251,252,\dots  ,1000\}$.}}{6}}
\newlabel{fig:figure_3}{{3}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Action-state function visualization for agent with Q-learning mechanism.}}{6}}
\newlabel{fig:figure_4}{{4}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Action-state function visualization for agent with Roth-Erev learning mechanism (standard learning process was switched off for it at $t_{0}=250$).}}{7}}
\newlabel{fig:figure_5}{{5}{7}}
\bibdata{bib/Whitehead2008,bib/Andrecut2002,bib/Arthur1994,bib/Challet1997,bib/Johnson2003,bib/Minority_Games,bib/Moro2004}
\bibcite{ANDRECUT2002}{AA02}
\bibcite{ARTHUR1994}{Art94}
\bibcite{CHALLET2005}{CMZ05}
\bibcite{CHALLET1997}{CZ97}
\bibcite{JOHNSON2003}{JJH03}
\bibcite{MORO2004}{Mor04}
\bibcite{WHITEHEAD2008}{Whi08}
\bibstyle{alpha}
