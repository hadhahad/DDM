\documentclass[12pt,american,czech]{article}
\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm,headheight=0.8cm,headsep=1cm,footskip=0.5cm}
\setcounter{secnumdepth}{3}
\usepackage{url}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{enumerate} %roman enumiration
\usepackage{threeparttable}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{subfigure}
\usepackage{array}
\usepackage[utf8]{inputenc} % Required for inputting international characters
\usepackage[T1]{fontenc} % Output font encoding for international characters
\usepackage{mathpazo} % Palatino font


\pagenumbering{arabic}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Algorithms
% Define a \HEADER{Title} ... \ENDHEADER block
\newcommand{\HEADER}[1]{\ALC@it\underline{\textsc{#1}}\begin{ALC@g}}
	\newcommand{\ENDHEADER}{\end{ALC@g}}
\renewcommand*{\ALG@name}{Algoritmus}
\algsetup{indent=2em} 
\renewcommand{\algorithmiccomment}[1]{\hspace{2em}// #1} 
\makeatother

%% Use Times New Roman font for text and Belleek font for math
%% Please make sure that the 'esint' package is turned off in the
%% 'Math options' page.
\usepackage[varg]{txfonts}


%% Indent even the first paragraph in each section
\usepackage{indentfirst}

% completely avoid orphans (first lines of a new paragraph on the bottom of a page)
\clubpenalty=9500

% completely avoid widows (last lines of paragraph on a new page)
\widowpenalty=9500

% disable hyphenation of acronyms
\hyphenation{CDFA HARDI HiPPIES IKEM InterTrack MEGIDDO MIMD MPFA DICOM ASCLEPIOS MedInria}

%%---------------------------------------------------------------------

%% Print out all vectors in bold type instead of printing an arrow above them
%%\renewcommand{\vec}[1]{\boldsymbol{#1}}

% Replace standard \cite by the parenthetical variant \citep
%\renewcommand{\cite}{\citep}

\makeatother
%\pagestyle{empty} %turns off page numbering
\usepackage{babel}
\newcommand*\Laplace{\mathop{}\!\mathbin\bigtriangleup}
\newcommand*\midpoint[1]{\overline{#1}}

\begin{document}
\selectlanguage{american}
\def\documentdate{...}


\begin{titlepage} % Suppresses displaying the page number on the title page and the subsequent page counts as page 1
	\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for horizontal lines, change thickness here
	\center % Centre everything on the page	
	
	\textsc{\LARGE FNSPE CTU}\\[1.5cm] % Main heading such as the name of your university/college
	\vfill
	
	\textsc{\Large Dynamic Decision Making}\\[0.5cm] % Major heading such as course name
	\textsc{\large Seminar Paper}\\[0.5cm] % Minor heading such as course title
	\HRule\\[0.4cm]
	{\huge\bfseries Minority Game}\\
	{\LARGE\bfseries From the Dynamic Decision Making Perspective}\\[0.4cm] % Title of your document
	\HRule\\[1.5cm]
	{\large\textit{Author}}\\
	Vladislav \textsc{Belov}\\
	\vfill\vfill\vfill\vfill\vfill\vfill\vfill % Position the date 3/4 down the remaining page
	{\large\today} % Date, change the \today to a set date if you want to be precise
	
	%------------------------------------------------
	%	Logo
	%------------------------------------------------
	
%	\vfill\vfill
%	\includegraphics[width=0.2\textwidth]{Images/TITLE/fjfi}\\[1cm] % Include a department/university logo - this will require the graphicx package
%	
	%----------------------------------------------------------------------------------------
	
	\vfill % Push the date up 1/4 of the remaining page
	
\end{titlepage}

%\tableofcontents
%\newpage{}

\section{Introduction}\label{sec:introduction}

In the following seminar paper the Minority Game is going to be discussed. In the next section we will define the general mathematical model of the Minority Game. Afterwards, agent Q-learning and Roth-Erev learning will be introduced. Within the scope of this work the implementation was performed and its results will be presented in some section.

\medskip
 
The system the Minority Game describes originates from the El Farol Bar Problem which was introduced by the economist W.B. Arthur [ref]. It goes as follows: every Thursday the population of Santa Fe has a desire to visit the bar: if more of 60 \% of people come to the bar, then it is considered to be overcrowded, so it is no fun there; if less, then the visit is pleasant and those who stayed at home are at a loss. Therefore, it can be easily seen, that the minority wins in games of El Farol Bar Problem type, that is the reason why the model has such a name. Nowadays this model is used frequently in Finance, Network Analysis, Biology, etc.

\section{Mathematical Model of the Minority Game}

Firstly, consider an odd $N=2k-1$, $k=1,2,\dots$, number of agents participating in the game. At each time step $t=1,2,\dots$ each agent has to make a decision whether to perform an action $+1$ (e.g. go to the bar, sell an asset on the market) or $-1$ (e.g. stay at home, buy an asset on the market). Formally designated $\forall i\in\{1,2,\dots,N\}$:

\begin{equation}
a_{i}(t)=\pm 1
\end{equation}

In order to study game dynamics a special parameter called \textit{total action} was introduced: 

\begin{equation}
A(t)=\sum_{i=1}^{N}a_{i}(t), \forall t\in\{1,2,\dots\},
\end{equation}

\noindent
which is basically the sum of actions performed by every agent at a given game round.

\medskip

After each game round the outcome is disclosed to each of the agents: as soon as the round is finished, everyone gets to know what the winning action was. This action $W(t+1)$ ($t+1$ is used to specify, that this information is available at a time step $t+1$) is determined by a simple rule:

\begin{itemize}
\item $A(t)>0$ $\implies$ the action $-1$ was victorious, $W(t+1)=-1$;
\item $A(t)<0$ $\implies$ the action $+1$ was victorious, $W(t+1)=1$;
\item $A(t)=0$ will never occur due to oddness of $N$.
\end{itemize} 

\noindent
In other words, $W(t+1) = -\text{sign}\big(A(t)\big)$.

\medskip

Minority  Game is a system of agents with bounded memory: each agent remembers $m\in\{1,2,\dots\}$ recent game outcomes. In accordance with its memory. This \textit{recent history} can be represented two different ways:

\begin{itemize}
	\item As an $m$-tuple of most recent game outcomes: $\bar{\mu}(t)=(W(t-m+1),W(t-m+2),\dots,W(t))$;
	\item As a single decimal number $\mu(t)$ with binary representation equal to $\bar{\mu}(t)$.\footnote{E.g.: let $m=3$, then for $\bar{\mu}(t)=(-1,-1,1)$ its decimal representation (obtained by applying the transformation $-1\rightarrow 0$) is $\mu(t)=1$, because $001_{2}=1_{10}$.}
\end{itemize} 

\noindent
It can be easily seen, that in total $2^{m}$ possible recent histories exist, therefore, $\mu(t)\in\{1,2,\dots,2^{m}-1\}$.

\medskip

At the beginning of the game each agent gets a fixed number of strategies. For any fixed $m$ a \textit{strategy} is a mapping $\pi:(0,1,\dots,2^m-1)\mapsto\{-1,1\}^{2^{m}}$ (all possible outcomes are mapped to respective actions, e.g. see Table~\ref{tab:strategy_table_example}). The construction of such mapping implies, that agents are able to get from $0$ to $2^{2^{m}}$ strategies. The set of strategies available to agent $i\in\{1,2,\dots,N\}$ will be called \text{agent's strategic portfolio} and denoted as $\mathcal{P}_{i}$. The action/decision of agent $i$ with respect to recent history $\mu(t)$ using the strategy $\pi$ will be denoted as $\pi_{i}(\mu(t))$, e.g. if agent $i$ uses the strategy from Table~\ref{tab:strategy_table_example} when recent history is $101$, then $\pi_{i}(5)= -1$.

\begin{table}
	\centering
	\begin{tabular}{|c|c||c|}
		\hline
		Recent History, $\bar{\mu}(t)$ & $\mu(t)$ & Action/Decision \\
		\hline
		\hline 
		$0 0 0$ & 0 & $-1$ \\ 
		$0 0 1$ & 1 & $+1$ \\ 
		$0 1 0$ & 2 & $+1$ \\ 
		$0 1 1$ & 3 & $-1$ \\ 
		$1 0 0$ & 4 & $+1$ \\ 
		$1 0 1$ & 5 & $-1$ \\ 
		$1 1 0$ & 6 & $+1$ \\ 
		$1 1 1$ & 7 & $+1$ \\
		\hline
	\end{tabular} 
	\caption{Strategy example for $m=3$.} 
	\label{tab:strategy_table_example}
\end{table}

Not the question arises: how to model the process of decision making? In the following section we will discuss the original learning mechanism and some other.

\section{Learning in the Minority Game}

To begin, we will try to describe the Minority Game in a more general way. It is a complex system where agents need memory to make an optimal decision and not all relevant portions of the environment can be observed - we are dealing with partially observable, stochastic, sequential, dynamic, discrete, multi-agent environment. Depending on the learning mechanism, the set of states can be defined differently. Here basic ideas of the MG representation as MDP will be highlighted and some other SHIT DONE.

Let $(\mathcal{S},\mathcal{A},\mathcal{T},\mathcal{R})$ be a Markov Decision Process representing the Minority Game. Obviously, both the transition model $\mathcal{T}$ and the reward model $\mathcal{R}$ are not available, so the agents have to learn "along the way".

In this paper we want to focus on comparison of different learning mechanism and what is their impact on the performance of a single agent. Let agent $i$ has won $w_{i}(t)$ and lost $l_{i}(t)$, $w_{i}(t)+l_{i}(t)=t$, then its performance at time $t$is defined as:

\begin{equation}
p_{i}(t) = w_{i}(t)-l_{i}(t).
\end{equation}

\subsection{Original Learning Mechanism}\label{subsec:original}

The original learning mechanism is based on giving strategies scores which will be updated as the game progresses, in other words, if the strategy was successful during the recent round, then its score is increased, and vice versa. The update rule of the score for strategy $\pi$ denoted as $R^{\,\pi}$ is defined as follows:

\begin{equation} \label{eq:mg_score_upd}
R^{\,\pi}(t+1) = R^{\,\pi}(t)-\delta_{\mu(t),j}\cdot\pi(\mu(t))\cdot g\big(A(t)\big)
\end{equation}

\noindent
where $\delta$ stands for Kronecker delta and $g$ is any odd function with respect to total action $A(t)$.\footnote{Here for simplicity $g(\cdot)=\text{sign}(\cdot)$.} 

At each game round agents choose the best scoring strategy from the ones available for them:

\begin{equation}
\pi_{i}^{\text{opt}} = \arg \max_{\pi\in\mathcal{P}_{i}} R(t).
\end{equation}

The strategy score can be in some sense interpreted as the reward function, but such an assumption is a bit misleading. 

\subsection{Q-learning Mechanism}\label{subsec:q}

The Q-learning approach is based on the update of Q-function which is an expected total reward from taking action $a\in\mathcal{A}$ at state $s\in\mathcal{S}$. Its update rule is defined as:

\begin{equation}
Q_{i}(s,a)\leftarrow(1-\alpha)\cdot Q_{i}(s,a)+\alpha\cdot\big(r(s)+\gamma\cdot\max_{a'}Q_{i}(s',a')\big)
\end{equation}

\noindent
where $s'$ is the next state, $\alpha$ is a learning parameter, $\gamma$ is a discount factor and $r$ is an immediate reward at state $s$.

Let $\mathcal{S}=\{\pi: \pi \text{ is a strategy}\}$, the set of possible actions will be defined as $\mathcal{A}=\{$switch from using strategy $\pi$ to $\tilde{\pi}$ : $\pi,\tilde{\pi}$ are strategies $\}$. Now Q-learning may be applied on the minority game. The action-state function denoted as $Q_{i,\pi,\tilde{\pi}}$ has the following meaning:

\begin{itemize}
\item If $\pi=\tilde{\pi}$, then the agent will not change his current strategy to another one;
\item If $\pi\neq\tilde{\pi}$, then the agent will switch from strategy $\pi$ to $\tilde{\pi}$.
\end{itemize}

Agent $i\in\{1,2,\dots,N\}$ will choose its optimal strategy at each time step by the $\epsilon$-greedy rule\footnote{Strategy $\pi_{i}^{\text{opt}}$ will be chosen with probability $1-\epsilon$.}

\begin{equation}
\pi_{i}^{\text{opt}} = \arg \max_{\tilde{\pi}\in\mathcal{P}_{i}} Q_{i,\pi,\tilde{\pi}}.
\end{equation}


Action-state function update rule which has to be evaluated at each game round will take form:

\begin{equation}
Q_{i,\pi,\tilde{\pi}}\leftarrow (1-\alpha)\cdot Q_{i,\pi,\tilde{\pi}}+\alpha\cdot\big(r(\pi)+\gamma\cdot\max_{\pi'}Q_{i,\pi,\pi'}\big)
\end{equation}

\noindent
where after using the strategy $\pi$ agent's immediate reward is $r(\pi)=-\pi_{i}(\mu(t))\cdot A(t)$. That means, in case $\pi$ was successful it will be rewarded by $\arrowvert A(t)\arrowvert$  and $-\arrowvert A(t)\arrowvert$ in the opposite case.

\subsection{Roth-Erev Learning Mechanism}\label{subsec:rerl}

Roth-Erev reinforcement learning is completely different from the techniques discussed in subsections \ref{subsec:original} and \ref{subsec:q}. Here agents are not strategies according to the original model, decision making is based on \textit{propensities} and their update rule. Although the state space $\mathcal{S}$ and the set of possible actions $\mathcal{A}$ are the same as in the subsection \ref{subsec:original}, the learning mechanism is as follows \cite{WHITEHEAD2008}:

\begin{itemize}

		\item Agent $i\in\{1,2,\dots,N\}$ has a propensity $q_{i}^{\,a}(t)$ to play action $a$, $\forall a\in\mathcal{A}$. Propensities are required to meet the condition $q>0$ and they have to stay positive\footnote{That can be achieved by defining a positive payoff function.};
		\item Let $(y_{i}(t), 1-y_{i}(t))$ represent agent's strategy at time $t$ which means, that he will take action $-1$ with probability $y_{i}(t)$, hence, the action $+1$ will be taken with probability $1-y_{i}(t)$. These probabilities are defined by a simple relation:
		
\begin{equation}
y_{i}(t)=Pr(\mathit{a}=+1)=\frac{q_{i}^{\mathit{+1}}(t)}{q_{i}^{+1}(t)+q_{i}^{-1}(t)}.
\end{equation}

		It is simple to deduce, that $Pr(\mathit{a}=-1)=1-y_{i}(t)$.
		
		\item Propensity is updated only if the taken action was successful\footnote{In the original text \cite{WHITEHEAD2008} the update is given as some general function. The following update was proposed to perform simulations.}:
		
\begin{equation}
q_{i}^{\mathit{a}}(t+1) \leftarrow q_{i}^{\mathit{a}}(t) + \frac{|A(t)|}{N}.
\end{equation}
\end{itemize}

\section{Simulations}


\newpage{}

\bibliography{}

%\bibliographystyle{plain}
\bibliographystyle{alpha}

\end{document}
